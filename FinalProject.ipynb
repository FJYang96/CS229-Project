{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils as util\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import exp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Load the feature header\n",
    "#######################################################\n",
    "import csv\n",
    "\n",
    "# read the csv file\n",
    "feature_names = []\n",
    "with open('./data/header.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        feature_names.append(line)\n",
    "\n",
    "# Dirty tricks to clean up the feature names\n",
    "feature_names = feature_names[0]\n",
    "feature_names = list(map(lambda x: x.replace(' ', '_'), feature_names))\n",
    "feature_names[0] = 'PharmGKB_Subject_ID' # manual fix to remove a character in f...[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Load the dataframe into X; clean up some unnecessary columns \n",
    "####################################################################\n",
    "\n",
    "# Load data and process column names\n",
    "df = pd.read_csv('./data/warfarin_with_dose.csv', names = feature_names)\n",
    "#keep_default_na=False)\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# Extract the label (Warfarin dose) from the rest of the features\n",
    "y = df.Therapeutic_Dose_of_Warfarin\n",
    "X = df.drop('Therapeutic_Dose_of_Warfarin',axis=1)\n",
    "feature_names.remove('Therapeutic_Dose_of_Warfarin')\n",
    "\n",
    "# Drop Subject_ID (irrelevant) and Medication (different to encode)\n",
    "X = X.drop('PharmGKB_Subject_ID',axis=1)\n",
    "X = X.drop('Medications',axis=1)\n",
    "X = X.drop('Comorbidities',axis=1)\n",
    "feature_names.remove('PharmGKB_Subject_ID')\n",
    "feature_names.remove('Medications')\n",
    "feature_names.remove('Comorbidities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Encode different features with numeric/label/onehot encodings \n",
    "####################################################################\n",
    "numeric_features = ['Height_(cm)',\n",
    "                    'Weight_(kg)',\n",
    "                    'Target_INR',\n",
    "                    'INR_on_Reported_Therapeutic_Dose_of_Warfarin',\n",
    "                    ]\n",
    "label_features =   ['Age',\n",
    "                    'Estimated_Target_INR_Range_Based_on_Indication']\n",
    "categorical_features = [f for f in feature_names \\\n",
    "                        if f not in numeric_features and f not in label_features]\n",
    "\n",
    "for feat in categorical_features:\n",
    "    X[feat] = X[feat].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "label_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='0-missing')),\n",
    "    ('ordinal', OrdinalEncoder())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('lab', label_transformer, label_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5528, 259)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val and test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.191186321644581e+17\n",
      "0.4912769925082705\n",
      "2.4616284861531695e+20\n",
      "31.52241807909605\n",
      "[ 1.03762222e+00  3.85513292e+00  3.98436247e-01  5.84098322e-01\n",
      " -2.16153503e+00 -1.91481316e-01 -1.73157801e+12 -1.73157801e+12\n",
      " -1.73157801e+12  2.47473474e+11  2.47473474e+11  2.47473474e+11\n",
      "  2.47473474e+11  7.62698577e+11  7.62698577e+11  7.62698577e+11\n",
      " -2.77169281e+09 -2.77169281e+09 -2.70410457e+11 -2.77169281e+09\n",
      " -2.77169281e+09 -7.25682033e+09 -2.77169281e+09 -2.77169282e+09\n",
      " -2.77169281e+09  2.45262330e+11 -2.77169282e+09 -2.77169281e+09\n",
      " -2.34518760e+10 -2.77169281e+09 -2.77169281e+09 -2.77169280e+09\n",
      " -1.76490996e+10 -2.77169282e+09 -2.77169282e+09 -2.77169282e+09\n",
      " -2.77169282e+09 -2.77169281e+09  3.17039372e+10 -2.77169282e+09\n",
      " -2.77169281e+09 -2.77169283e+09 -2.77169281e+09 -2.77169281e+09\n",
      " -2.77169282e+09 -2.77169284e+09 -2.77169282e+09 -2.77169282e+09\n",
      " -2.77169283e+09 -2.77169282e+09 -2.77169282e+09 -2.77169282e+09\n",
      " -2.77169283e+09 -2.77169282e+09 -2.77169281e+09 -2.77169279e+09\n",
      " -2.77169283e+09 -2.77169281e+09  3.39086781e+09 -2.77169282e+09\n",
      " -2.77169282e+09 -2.77169282e+09 -2.77169282e+09  2.73940092e+09\n",
      " -2.77169282e+09 -2.77169281e+09 -2.77169281e+09 -2.77169281e+09\n",
      " -2.77169282e+09 -3.43686560e+11 -3.43686560e+11 -3.43686560e+11\n",
      "  6.61142764e+10  6.61142764e+10  6.61142764e+10 -3.24147968e+10\n",
      " -3.24147968e+10 -3.24147968e+10  6.08946477e+11  6.08946477e+11\n",
      "  6.08946477e+11  4.17485869e+11  4.17485869e+11  4.17485869e+11\n",
      "  1.77287448e+11  1.77287448e+11  1.77287448e+11 -2.36717500e+11\n",
      " -2.36717500e+11 -2.36717500e+11  3.35334814e+09  3.35334814e+09\n",
      "  3.35334813e+09  3.15294136e+11  3.15294136e+11  2.95365878e+11\n",
      " -7.98104059e+11 -7.98104059e+11 -7.98104059e+11  1.71410246e+11\n",
      "  1.71410246e+11  1.71410246e+11  7.41979710e+11  7.41979710e+11\n",
      "  7.41979710e+11  1.47012914e+10  1.47012914e+10 -4.48923145e+11\n",
      " -4.48923145e+11 -4.48923145e+11 -1.88165082e+11 -1.88165082e+11\n",
      " -1.88165082e+11 -1.27640984e+12 -1.27640984e+12 -3.39448981e+11\n",
      "  3.95019249e+11  3.95019249e+11 -1.55454066e+10 -1.32242936e+12\n",
      " -1.32242936e+12 -1.32242936e+12  1.67005505e+11  1.67005505e+11\n",
      " -3.39462437e+11 -1.81193578e+12 -1.81193578e+12 -1.81193578e+12\n",
      " -2.14875101e+11 -2.14875101e+11 -2.14875101e+11 -2.80973966e+11\n",
      " -2.80973966e+11 -6.01949622e+11 -6.01949622e+11 -6.01949622e+11\n",
      "  2.80253940e+10  8.50825571e+10  5.40656668e+01  8.81174849e+10\n",
      "  2.80253940e+10  2.80253940e+10  9.54842025e+10  8.47357146e+10\n",
      "  2.80253940e+10  2.80253940e+10  9.21497538e+10  2.80253940e+10\n",
      "  2.52318881e+11  6.78093819e+10  1.37278019e+11  3.54730423e+10\n",
      "  5.38875557e+10  9.74884118e+10  3.33976767e+10  2.10096363e+11\n",
      "  5.36379662e+09  1.89873295e+11 -3.82370595e+10  1.20404659e+11\n",
      "  1.46272439e+11  2.58536756e+10  2.15321751e+11 -1.17042385e+12\n",
      " -1.17042385e+12 -1.17042385e+12 -1.17042385e+12  1.91669536e+11\n",
      "  2.37302785e+11 -5.17105900e+09  2.15321751e+11  2.86001833e+11\n",
      "  2.86001833e+11  2.86001833e+11  2.86001833e+11  4.47900719e+11\n",
      "  4.47900719e+11  4.47900719e+11  2.15321751e+11 -7.95187056e+09\n",
      " -7.95187056e+09 -7.95187057e+09 -7.95187057e+09  5.21298729e+11\n",
      "  2.78824884e+11  3.24458134e+11  2.15321751e+11 -9.93612941e+11\n",
      " -9.93612941e+11 -9.93612941e+11 -9.93612941e+11  2.35773404e+11\n",
      "  2.35773404e+11  2.35773404e+11  2.15321751e+11  7.97589716e+11\n",
      "  7.97589716e+11  7.97589716e+11  7.97589716e+11  9.97127985e+10\n",
      "  9.97127985e+10  9.97127985e+10  2.10909742e+11  1.19183560e+12\n",
      "  1.19183560e+12  1.19183560e+12  1.19183560e+12  1.36842269e+11\n",
      "  1.36842269e+11  1.36842269e+11  2.14839193e+11  1.90280328e+11\n",
      "  1.90280328e+11  1.90280328e+11  1.90280328e+11  3.59249410e+10\n",
      "  3.59249410e+10  3.59249410e+10  3.59249410e+10  1.50280860e+11\n",
      "  9.32236973e+10  0.00000000e+00  9.01887694e+10  1.50280860e+11\n",
      "  1.50280860e+11  8.28220519e+10  9.35705398e+10  1.50280860e+11\n",
      "  1.50280860e+11  8.61565005e+10  1.50280860e+11 -6.64229150e+10\n",
      " -6.64229149e+10 -6.64229149e+10 -6.64229150e+10 -1.14463749e+11\n",
      " -1.14463749e+11 -1.14463749e+11 -1.14463749e+11 -2.30290936e+11\n",
      " -2.30290936e+11 -2.30290936e+11 -2.30290936e+11  1.23161806e+12\n",
      "  1.23161806e+12  1.23161806e+12  1.23161806e+12  7.25008160e+11\n",
      "  7.25008160e+11  7.25008160e+11  7.25008160e+11  1.37094064e+12\n",
      "  1.37094064e+12  1.37094064e+12  1.37094064e+12  5.00409003e+11\n",
      "  5.00409003e+11  5.00409003e+11  5.00409003e+11]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_regressor = LinearRegression(fit_intercept = True)\n",
    "lr_regressor.fit(X_train, y_train)\n",
    "print(lr_regressor.score(X_val, y_val))\n",
    "print(lr_regressor.score(X_train, y_train))\n",
    "\n",
    "y_pred = lr_regressor.predict(X_val)\n",
    "print(mean_squared_error(y_val,y_pred))\n",
    "print(np.mean(y_val))\n",
    "coef = lr_regressor.coef_\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_regressor = SVR(kernel='rbf', gamma='auto')\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "svm_regressor.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVM with various hyperparameters\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#initialise arrays to store the scores \n",
    "svm_score_train = np.zeros((7,1))\n",
    "svm_score_val = np.zeros((7,1))\n",
    "\n",
    "for i, C in enumerate((1000,100,10,1, 0.1, 0.01,0.001)):\n",
    "    svm_regressor = SVR(kernel='rbf', gamma='auto', C=C)\n",
    "    svm_regressor.fit(X_train,y_train)\n",
    "    svm_score_train[i,:] = svm_regressor.score(X_train,y_train)\n",
    "    svm_score_val[i,:] = svm_regressor.score(X_val,y_val)\n",
    "    \n",
    "print('training score \\n', svm_score_train)\n",
    "print('validation score \\n', svm_score_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score \n",
      " [0.49127676 0.49127608 0.49122248 0.48984508 0.48414958 0.4677943\n",
      " 0.42004327]\n",
      "validation score \n",
      " [0.44686551 0.44705458 0.44862756 0.45435222 0.45738596 0.44720356\n",
      " 0.41238118]\n",
      "12.805426482489523\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#initialise arrays to store the scores \n",
    "ridge_score_train = np.zeros(7)\n",
    "ridge_score_val = np.zeros(7)\n",
    "\n",
    "for i, C in enumerate((1000,100,10,1, 0.1, 0.01,0.001)):\n",
    "    ridge_regressor = Ridge(alpha=1/C, fit_intercept = True)\n",
    "    ridge_regressor.fit(X_train,y_train)\n",
    "    ridge_score_train[i] = ridge_regressor.score(X_train,y_train)\n",
    "    ridge_score_val[i] = ridge_regressor.score(X_val,y_val)\n",
    "    \n",
    "print('training score \\n', ridge_score_train)\n",
    "print('validation score \\n', ridge_score_val)\n",
    "\n",
    "best_ridge = Ridge(alpha = 1, fit_intercept = True)\n",
    "best_ridge.fit(X_train,y_train)\n",
    "y_pred = best_ridge.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_pred)))\n",
    "# print(np.mean(y_val))\n",
    "\n",
    "# print(best_ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xaxis = [0.001,0.01,0.1,1,10,100,1000] #values of lambda used\n",
    "plt.plot(xaxis,ridge_score_train, label = 'training')\n",
    "plt.plot(xaxis,ridge_score_val, label = 'validation')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.title('Accuracy vs Lambda - Ridge Penalty')\n",
    "plt.savefig('Ridge.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhiqi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score \n",
      " [0.49057317 0.482041   0.44870825 0.34330584 0.         0.\n",
      " 0.        ]\n",
      "validation score \n",
      " [ 0.45273163  0.45458929  0.42456451  0.34166222 -0.00081468 -0.00081468\n",
      " -0.00081468]\n",
      "12.802644426578187\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#initialise arrays to store the scores \n",
    "lasso_score_train = np.zeros(7)\n",
    "lasso_score_val = np.zeros(7)\n",
    "\n",
    "for i, C in enumerate((1000,100,10,1, 0.1, 0.01,0.001)):\n",
    "    lasso_regressor = Lasso(alpha=1/C, fit_intercept = True)\n",
    "    lasso_regressor.fit(X_train,y_train)\n",
    "    lasso_score_train[i] = lasso_regressor.score(X_train,y_train)\n",
    "    lasso_score_val[i] = lasso_regressor.score(X_val,y_val)\n",
    "    \n",
    "print('training score \\n', lasso_score_train)\n",
    "print('validation score \\n', lasso_score_val)\n",
    "\n",
    "best_lasso = Lasso(alpha = 0.01, fit_intercept = True)\n",
    "best_lasso.fit(X_train,y_train)\n",
    "y_pred = best_lasso.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,y_pred)))\n",
    "# print(np.mean(y_val))\n",
    "\n",
    "# print(best_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xaxis = [0.001,0.01,0.1,1,10,100,1000] #values of lambda used\n",
    "plt.plot(xaxis,lasso_score_train, label = 'training')\n",
    "plt.plot(xaxis,lasso_score_val, label = 'validation')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('score')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.title('Accuracy vs Lambda - Lasso Penalty')\n",
    "plt.savefig('Lasso.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447289218375866\n",
      "0.6281061493376572\n",
      "166.10153294337277\n",
      "31.52241807909605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhiqi\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor() #use default settings\n",
    "nn_regressor.fit(X_train, y_train)\n",
    "print(nn_regressor.score(X_val, y_val))\n",
    "print(nn_regressor.score(X_train, y_train))\n",
    "\n",
    "y_pred = nn_regressor.predict(X_val)\n",
    "print(mean_squared_error(y_val,y_pred))\n",
    "print(np.mean(y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-nested cross validation using RBF SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rbf = SVR(kernel='rbf')\n",
    "gammas = np.logspace(-6, 0, 7)\n",
    "params = {'gamma': gammas}\n",
    "gridcv = GridSearchCV(estimator=rbf, param_grid=params, cv=5)\n",
    "gridcv.fit(X_trainval, y_trainval)\n",
    "scores_mean = gridcv.cv_results_['mean_test_score']\n",
    "scores_sd = gridcv.cv_results_['std_test_score']\n",
    "print('highest accuracy score is',gridcv.best_score_)\n",
    "model = gridcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-nested cross validation using Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_regressor = Ridge(fit_intercept = True)\n",
    "params={'alpha': [25,10,4,2,1.0,0.8,0.5,0.3,0.2,0.1,0.05,0.02,0.01]}\n",
    "\n",
    "gridcv = GridSearchCV(estimator=ridge_regressor, param_grid=params, cv=10)\n",
    "gridcv.fit(X_trainval, y_trainval)\n",
    "scores_mean = gridcv.cv_results_['mean_test_score']\n",
    "scores_sd = gridcv.cv_results_['std_test_score']\n",
    "print('highest accuracy score is',gridcv.best_score_)\n",
    "model = gridcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672316384180791"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy(lr_regressor, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6711864406779661"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy(best_ridge, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6655367231638418"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy(best_lasso, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7717514124293785"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy_two_classes(best_ridge, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728813559322034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy_two_classes(best_lasso, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728813559322034"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_util.val_bin_accuracy_two_classes(nn_regressor, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generates detailed results in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7717514124293785\n",
      "334\n",
      "132\n",
      "349\n",
      "70\n",
      "885\n"
     ]
    }
   ],
   "source": [
    "loss, TP, FP, TN, FN = exp_util.val_bin_accuracy_with_con_matrix(best_ridge, X_val, y_val)\n",
    "print(loss)\n",
    "print(TP)\n",
    "print(FP)\n",
    "print(TN)\n",
    "print(FN)\n",
    "\n",
    "print(TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7717514124293785\n",
      "0.8267326732673267\n",
      "0.7255717255717256\n",
      "0.7167381974248928\n",
      "0.8329355608591885\n"
     ]
    }
   ],
   "source": [
    "accuracy, sensitivity, specificity, precision_plus, precision_minus =exp_util.get_classification_metrics(\n",
    "    best_ridge, X_val, y_val)\n",
    "print(accuracy) #benchmark = 66%\n",
    "print(sensitivity) #benchmark = 63%\n",
    "print(specificity) #benchmark = 73%\n",
    "print(precision_plus) #benchmark = 81%\n",
    "print(precision_minus) #benchmark = 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While we seem to do better in many of the metrics, we obtained lower precision (plus). Interesting.\n",
    "# To discuss, which metric is more important in our context? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
